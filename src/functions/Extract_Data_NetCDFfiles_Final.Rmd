---
title: "Extract_Data_NetCDFfiles2"
author: "Gustavo Facincani Dourado"
date: "8/12/2020"
output: html_document
---

Here we are going to create a function to extract all data of interest within a basin's area from NetCDF files.
In this case, I'll use "Mer.shp", "Tuo.shp", "Stn.shp", "USJ.shp", which are files that contain multiple polygons (subbasins), for the watersheds: Merced, Tuolumne, Stanislaus and Upper San Joaquin. I used the file "allbasin_subwatersheds" we have on box (the 4 basins are together, but I split the shapefile into 4 using QGIS).
To use this function, the projections of the NetCDF and shapefile need to be the same, in our case, they are. Projections can be checked with csr(). The function will return me .csv files containing daily data for each subbasin in separate files, in folders named after each basin, GCM and RCP. It takes a long time to read these files.
Units of the NetCDF file can be checked by using nc_open():
tot_runoff & runoff & rainfall & precip & baseflow & del_SWE & snowmelt &  snowfall & ET = mm/day; 
SWE & soilMoist1, 2 & 3 = mm; 
Tair = °C.

Setting global variables

```{r}

rcps <- c("rcp45", "rcp85") # Emission scenarios we are interested in

tot_variables <- c("ET", "Tair", "baseflow", "precip", "rainfall", "SWE", "runoff", "snow_melt", "snowfall", "tot_runoff")

needed_variables <- c("tot_runoff")

tot_GCMs <- c("CanESM2", "CNRM-CM5", "HadGEM2-ES","MIROC5", "ACCESS1‐0",
              "CCSM4","CESM1-BGC","CMCC-CMS","GFDL-CM3","HadGEM2-CC")

used_GCMs <- c("CanESM2", "CNRM-CM5", "HadGEM2-ES","MIROC5")

needed_GCMs <- c( "ACCESS1‐0","CCSM4","CESM1-BGC","CMCC-CMS","GFDL-CM3",
                  "HadGEM2-CC")

# Path to shapefiles that are used
shp_path <- "C:/Users/gusta/Desktop/PhD/CERCWET/GCMs/Shapefiles/"

# Path to save the csv files

csv_path <- paste0("C:/Users/gusta/Box/VICELab/RESEARCH/PROJECTS/
  CERC-WET/Task7_San_Joaquin_Model/Pywrmodels/data/",basin,"/","hydrology/   gcms","/",GCM,"_",rcp,"/")

#if the directory doesn't exist, make it!
if (!dir.exists(csv_path)){
  dir.create(file.path(csv_path), recursive = TRUE)
}

```


```{r}

NetCDF_Extract <- function(shapefile, basin) { 
  
  # Basin = name of folder in which you'll save the data, such as "Merced River" #shapefile = name of the shapefile you'll use, as mentioned in line 9
  
  # Let's loop through 2006-2099
  
  # If you want just one GCM, one RCP scenario or one variable, you can just "mute" (#) the undesired ones by commenting them out in the first lines of this function, before the loop for the years
  
  for (GCM in GCMs){
    for(rcp in rcps) {
      for(variable in variables) {
        for(year in 2006:2099){
          # Read in the netCDF data 
          dpath <- paste0(wd,"/",GCM, "/",rcp,"/",variable,".",
                          as.character(year),".v0.CA_NV.nc")
          
          file <- brick(dpath)
          # Give me the number of layers in netCDF data
          nl <- nlayers(file) 
          
          # Set the dataframe we want, to be a list, in order to store all results of the loops
          df = list() 
          
          #Set the subbasin we want, to be a list, in order to store all results of the loops
          subbasin = list() 
          
          for (layers in 1:nl){
            
            # Extract the raster file of layers
            r <- raster(file, layer = layers)
            
            # Extract relevant information from shapefile
            shp_file <- shapefile(paste0(shp_path,shapefile))
            shp_file$SUBWAT <- gsub("^.{0,4}", "sb", shp_file$SUBWAT) #switch names in the attribute table of the shp file (example: MER_O1, MER_02, etc, will become sb01, sb02, etc)
            #these shapefiles have basin names as MER_01, MER_02, etc. So, here I'm selecting the first 4 digits, and switching them for "sb", so that I can use this attributes later to split the .csv file directly by subbasin, already with the same labels we currently have on Box
            Shp <- shp_file["SUBWAT"]
            Shp_file$AREA <- area(Shp_file) #get the area of the shapefiles in squared meters into the attribute table
            
            # Extract the mean value of cells within the polygons
            # Alternative: look to "mask" function ?mask
            masked_file<-raster::extract(r, #extract function is very computationally heavy
                                 Shp, #shapefile
                                 fun = mean, #this gives the mean observed values in the region, if fun = NULL, we will have values for each point, with the respective weight for each point
                                 na.rm=TRUE, 
                                 df=T, #as a dataframe
                                 small=T, #return a number, also when the buffer does not include the center of a single cell
                                 sp=T,  #extracted values are added to the data.frame
                                 weights=TRUE, #the function returns, for each polygon, a matrix with the cell values and the approximate fraction of each cell that is covered by the polygon
                                 normalizedweights=TRUE) #weights are normalized (they add up to 1 for each polygon)
            
            # Generate variable depicting the date
            # Extract the information about the time
            Date <- r@z[[1]] #same name as the other files we have on Box
            Date
            Date <- as.Date(Date, origin = "1800-01-01")
            
            
            # Compile flow and time variable in one dataframe
            df <- data.frame(Date, masked_file)
            
            # rename column with the variable that represents the extracted values
            colnames(df)[3] <- "flow" #same name as the one we have on Box (correcting typo "flw" of the previous versions) 
            
            for(j in unique(df$SUBWAT)) { #select the data for each subbasin separately
              
              subbasin <- df %>% 
              subset(., SUBWAT == j) %>% #subsetting per subbasin
              #here I'm diving by 1,000 and 86400 seconds to make mm/day become m/s, then multiplying by the area of the subbasin (m2) to get m3/s
              mutate(flow = (flow/1000/86400)*shp_file$AREA[shp_file$SUBWAT == j],
              
              # save data as .csv
              write.table(subbasin, #vector we want to save
                          file= paste0(csv_path, "tot_runoff_",j,".csv"), #save csv files per basin using the basin "name"
                          append=TRUE, #this gathers all the loop's results, if FALSE we'll have results being overwritten over and over again in the first line
                          sep=",",
                          col.names=!file.exists(paste0(csv_path,"tot_runoff_",j,".csv")), #if we set as TRUE, we'll have headings repeated per each row, in this way we just have one heading
                          row.names=FALSE, #no names for rows
                          quote = FALSE) #no column with quotes
              
            }
          }
        }
      }
    }
  }
}

```

Running the extract function for all basins

```{r}
NetCDF_Extract("Mer.shp", "Merced River")
NetCDF_Extract("Tuo.shp", "Tuolumne River")
NetCDF_Extract("USJ.shp", "Upper San Joaquin River")
NetCDF_Extract("Stn.shp", "Stanislaus River")
```
